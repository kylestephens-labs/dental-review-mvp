Here’s crisp, actionable feedback to make Task 2.5 tighter, more realistic, and directly aligned with your MCP Orchestrator spec and SLAs.

⸻

What’s great
	•	Clear intention to maximize parallelism (6× Cursor, 18 tasks).
	•	Thoughtful modules (capability matrix, LB, conflict resolver, QoS gates).
	•	Strong ops posture (monitoring, autoscaling, runbooks).

⸻

Gaps / Risks & How to fix them

1) Scope vs. Spec (MVP vs Phase 2)
	•	Issue: “Enterprise-grade” features (autoscaling, predictive AI, dashboards) belong to Phase 2 in your spec. MVP ACs focus on assignment <5s, 20 concurrent tasks (AC2.5), basic monitoring, escalation.
	•	Fix: Re-scope T2.5 to Phase 2 or split into 2.5a (MVP) and 2.5b (P2). Keep MVP lean: deterministic scheduling, basic metrics, no autoscaling/ML.

2) Concurrency targets conflict with spec
	•	Issue: You target 18 concurrent tasks (6×3), while AC2.5 says up to 20 concurrent tasks with no degradation.
	•	Fix: Set pool capacity ≥ 20. If you keep 6 agents, make per-agent concurrency ceil(20/6)=4 for 24 headroom, then cap effective concurrency at 20 via queue backpressure.

3) Memory/SLA mismatch
	•	Issue: Proposal assigns 3 GB RAM per Cursor. Spec’s Performance Validation requires <512 MB RAM under normal load (system-wide for the orchestrator).
	•	Fix: Clarify: those per-Cursor resources describe external agents (outside orchestrator SLOs). For the orchestrator, budget ≤512 MB at 20 concurrent tasks. Add memory guards (heap limit flag, circuit breakers, bounded queues).

4) “Auto-scaling Cursor” feasibility
	•	Issue: Programmatically spawning Cursor IDE instances is likely out-of-band / non-portable.
	•	Fix: Rename to “agent worker processes/containers” with a pluggable Agent Adapter (Cursor, ChatGPT, Codex). Provide scaling hooks, not hard-coded Cursor spawning.

5) “AI-powered” features need deterministic fallback
	•	Issue: “AI-powered capability matrix / scoring” is great, but MVP needs deterministic, testable rules.
	•	Fix: Implement weighted heuristic (priority → capability → load → fairness). Leave ML scoring as feature-flagged for Phase 2.

6) Conflict resolution details
	•	Issue: “Intelligent merge” is underspecified and hard to validate.
	•	Fix: Start with deterministic strategies:
	•	File-level exclusivity on hot files (lease w/ TTL).
	•	Git 3-way merge with pre-checks; escalate on same-line diffs.
	•	Dependency queueing (AC3.4 Phase 2) with explicit requeue reason.
	•	Emit OrchestratorEvent on each branch.

7) Monitoring & Updates
	•	Issue: Full dashboard is heavy for MVP; your spec mandates progress updates every 2h (AC2.4) and /healthz.
	•	Fix: MVP: Prometheus metrics + /metrics, /healthz, and a 2h notifier. Full dashboard to Phase 2.

8) Acceptance criteria should be measurable
	•	Issue: Several AC items are qualitative (e.g., “enterprise-grade”).
	•	Fix: Replace with numerical thresholds: p95 assignment <5s; headroom; error budgets; escalation ≤5m for unresolvable conflicts.

9) API & Events compliance
	•	Issue: Ensure all new actions produce events using the existing schema.
	•	Fix: For every decision (assign, reassign, conflict, escalate) emit a typed OrchestratorEvent and add table-driven tests.

⸻

Suggested rewrite: split and tighten

Task 2.5a (MVP) — Parallel Streams with deterministic scheduling

Task Title: T2.5a: Deterministic parallel scheduling (≥20 concurrent), SLA-compliant
Overview: Implement a deterministic scheduler supporting at least 20 concurrent tasks across available agents with capability-aware routing, dependency blocking, and P0-first priority, staying within orchestrator RAM <512 MB.
Goal: Meet AC1.1–AC1.4, AC2.1–AC2.4, AC3.1–AC3.3, AC4.1–AC4.2, AC5.x MVP, and Performance Validation/SLA.

Acceptance criteria (measurable):
	•	Concurrency: Support ≥20 concurrent tasks without p95 assignment degradation >5s at 100 req/min (5 min).
	•	Scheduling policy: Weighted heuristic: score = (priorityWeight*P) + (capabilityMatch*Wc) + (1-load)*Wl + (fairness)*Wf. Document weights.
	•	Dependencies: Block tasks with unmet deps; no assignment until deps complete.
	•	P0 priority: P0 always scheduled ahead of P1/P2 (prove with test fixtures).
	•	Multiple P1 to Cursor: When Cursor is available, assign ≥2 independent P1 tasks in parallel.
	•	Agent status: Heartbeats update availability; unresponsive ≥10m triggers reassignment within ≤10s (AC4.2/AC3.3).
	•	Memory guard: Orchestrator process stays <512 MB RSS at 20 concurrent tasks (asserted in benchmark logs).
	•	Progress updates: Emit 2-hour updates (configurable), include active tasks, blocked reasons (AC2.4).
	•	Events: All transitions emit OrchestratorEvent entries per schema.

Definition of done:
	•	Benchmarks: npm run benchmark → p95 assignment <5s; RSS <512 MB; 0 assignment errors.
	•	Load test: npm run test:orchestrator:load @100 req/min × 5 min → no dropped assignments; throughput stable.
	•	Tests: Unit+integration coverage ≥60% for scheduler, queue, conflicts, heartbeat/failover.
	•	Docs: Scheduling policy, weights, configuration, ops runbook.

How cursor will validate DoD:
	•	Run scripts/validate-dod.sh chaining tests, load, benchmarks; parse for thresholds and exit non-zero on failure.
	•	Inspect /metrics for concurrency, queue depth, assignment latency.

Approximate size: 8–12 files, ~2–3 MB.

⸻

Task 2.5b (Phase 2) — Advanced autoscaling & predictive features (feature-flagged)

Task Title: T2.5b: Feature-flagged autoscaling, predictive scoring, dashboard
Overview: Add ML/AI scoring, autoscaling hooks for agent worker pools (not IDE instances), and Grafana dashboards.
Goal: Fulfill AC2.5, AC3.4–AC3.5, AC4.3–AC4.5, Phase 2 observability.

Acceptance criteria:
	•	Autoscaling hooks: Scale agent workers by queue depth, p95 assignment, and CPU utilization (targets documented). Dry-run mode on by default.
	•	Predictive scoring (opt-in): ML-assisted score blended with deterministic baseline; must not degrade p95 assignment latency vs. baseline in A/B test.
	•	Advanced conflict handling: Automatic dependent task queueing (AC3.4). Unresolvable conflicts escalate ≤5m (AC3.5).
	•	Workload balancing: No single agent >150% of median load over 10-min windows when alternatives exist (AC4.3).
	•	Dynamic agent registry: Hot add/remove agents without downtime (AC4.5), behind a flag.
	•	Dashboards: Prometheus + Grafana with panels for queue depth, assignment latency (p50/p95), stalls, escalations, heartbeats, memory.

Definition of done:
	•	A/B benchmark report: predictive vs deterministic; no SLA regression.
	•	Playbook for scaling policies; safe rollback.
	•	Screenshots/configs for dashboards checked in.

Approximate size: 10–15 files, ~3–5 MB.

⸻

Concrete tweaks to your snippets
	1.	Scoring: keep your calculateAssignmentScore but remove async I/O in hot path (use cached windows) and document weights:

// Weights: priority 0.5, capability 0.3, load 0.15, fairness 0.05 (configurable)
score = 0.5*priorityRank + 0.3*capabilityMatch + 0.15*(1-currentLoad) + 0.05*fairnessBoost;

	2.	Conflict policy (deterministic first):

	•	If same file & overlapping hunks → escalate.
	•	If same file & disjoint hunks → git 3-way merge; if merge clean, allow; else escalate.
	•	If dependency not met → auto-queue (Phase 2).
	•	Always emit events with conflict_type, resolution.

	3.	Memory/throughput guards:

	•	Use bounded queues (e.g., maxQueue=1000, backpressure 429 on overflow).
	•	Circuit breakers for agent adapters; exponential backoff.
	•	Idempotency keys for task submissions to avoid duplicates (AC3.1).

	4.	Monitoring (MVP):

	•	Export /metrics (Prometheus): assignment_latency_ms_bucket, queue_depth, active_tasks, agent_heartbeat_age_seconds, reassignments_total, conflicts_total.
	•	Keep the “dashboard” for Phase 2; start with basic graphs.

	5.	Terminology/feasibility:

	•	Replace “spawn Cursor” with “scale agent workers”.
	•	Clarify that external agents’ resource profiles don’t count toward orchestrator RAM SLA.

	6.	Test fixtures:

	•	Provide synthetic workloads: {mix: 40% P0, 40% P1, 20% P2}, deps chains length 1–3, hot-file conflicts rate 5–10%.
	•	One scenario with 20+ concurrent to hit AC2.5.

⸻

Shortened, optimized Task 2.5 (drop-in)

Task Title: T2.5: Deterministic Parallel Streams (20-concurrency, SLA-safe)
Overview: Implement a deterministic, capability-aware scheduler and worker pool supporting ≥20 concurrent tasks with P0-first assignment, dependency blocking, and failover ≤10s, while keeping orchestrator RAM <512 MB.
Goal: Meet MVP ACs and Performance Validation without Phase-2 bloat.

Acceptance criteria (must all pass):
	1.	Concurrency: Handle 20 concurrent tasks with p95 assignment <5s at 100 req/min (5 min).
	2.	Priority & capability: P0 always precedes P1/P2; tasks routed to agents meeting required capability.
	3.	Dependencies: Block until all deps completed; no premature assignment.
	4.	Multi-assign to Cursor: ≥2 independent P1 tasks assigned in parallel to Cursor when available.
	5.	Failover: Agent unresponsive ≥10m → reassigned ≤10s.
	6.	Memory: Orchestrator RSS <512 MB during load test.
	7.	Events: Emit task_assigned, task_reassigned, conflict_detected, agent_failed using existing schema.
	8.	Progress: Summaries emitted every 2h with active/blocked counts.

Definition of Done:
	•	npm run test:orchestrator & npm run test:integration pass, coverage ≥60% core.
	•	npm run test:orchestrator:load meets p95 and no drops.
	•	npm run benchmark logs memory and latency under thresholds.
	•	/healthz 200; /metrics exposes required series.
	•	Runbook updated; config documented.

How cursor validates DoD: Run scripts/validate-dod.sh; fail on any threshold breach.

⸻

If you want, I can refactor your current Task 2.5 doc into 2.5a/2.5b in your MCP task list (with crisp ACs/DoD above) and plug in the metrics & event names so it’s instantly actionable.